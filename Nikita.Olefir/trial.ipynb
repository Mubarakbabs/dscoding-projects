{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from TextAnalysis.comments_extractor import comments_extractor\n",
    "from TextAnalysis.preprocessor import TextPreprocessor\n",
    "from TextAnalysis.descriptive_statistics import word_length_distribution, calculate_word_statistics, plot_most_frequent_words\n",
    "from TextAnalysis.words_replacing import WordReplacer\n",
    "from TextAnalysis.stem_and_lem  import TextStemLem\n",
    "from TextAnalysis.pre_prepocessing import hash_text, change_time_format\n",
    "from TextAnalysis.word_mapping import word_mapping\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exctraction of comments using Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two distinct dataframes are created. It takes a few minute to extract all comments, \n",
    "# so consider making tea in the meantime\n",
    "\n",
    "comments_after_win = comments_extractor('ACMilan', '17pzwvv') \n",
    "comments_after_lose = comments_extractor('ACMilan', '17gb1xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_after_lose = pd.read_csv('milan_lost.csv')\n",
    "comments_after_win = pd.read_csv('milan_win.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that everything is ok just by writing the name(s) of a DataFrame and running the code. Before that, however, I want to apply two functions that allow me to present the better version of the data:\n",
    "- I want to hash the names of people leaving commets\n",
    "- change the date_time representation so that it is more user-friendly\n",
    "\n",
    "To do so, I use functions from the \"pre_preprocessing\" module from the TextAnalysis Folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly I hash the 'author' columns in both dataframes\n",
    "comments_after_win = hash_text(comments_after_win,column_name='author')\n",
    "comments_after_lose = hash_text(comments_after_lose,column_name='author')\n",
    "\n",
    "# Then I change the format of the representation of date and time\n",
    "comments_after_win = change_time_format(comments_after_win,column_name='created_utc')\n",
    "comments_after_lose = change_time_format(comments_after_lose,column_name='created_utc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrames afrer pre_prepocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_after_lose.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_after_win.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you do not want to run the same command over and over again, consider downloading data \n",
    "to a csv file and name it the way you like. After that just assign in to a variable. For example:\n",
    "\n",
    "```py\n",
    "comments_after_lose.to_csv('milan_lost.csv', index=False)\n",
    "sad_comments = pd.read_csv('milan_lost.csv')\n",
    "```\n",
    "\n",
    "I will continue with the previous name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we use quite a few function to preprocess data\n",
    "Consult with the \"preprocessor\", \"words_replacing\"and \"stem_and_lem\" modules in the TextAnalysis folder. All the documentation is written there.\n",
    "\n",
    "After the execution of the comments we expect the text to be:\n",
    "- In lower case;\n",
    "- Without any links;\n",
    "- Without punctuation;\n",
    "- Without stopwords (words that don't have much meaning);\n",
    "- Without any special characters;\n",
    "- Contaning only words with more than 2 letters (to avoid nonsense words that might appear during previous stages);\n",
    "- With curse words masked;\n",
    "  \n",
    "Additionally, I manually replace some of the words as long as they refer to the same thing(person) but are treated differently when analyzing data. Mainly, it concerns surnames of the players. For example, Christian Pulisic can be addressed by \"Puli\" and \"Pulisich\" by the fans of Milan; or \"RCL\" is a \"Ruben Loftus-Cheek\". No python library can trace it, so I have to manually change it if I notice something. It can surely impact the analysis of the comments, but there is no recearch withou limitations.  For that, I use \"words_replacing\" module in the TextAnalysis folder.\n",
    "\n",
    "Lastly, I want to add two columns to the dataframe: one with stemmed words and another with lemmatized words. Those are nedded as I want to compare text of I receive after preprocessing with even \"purer\" data to have just a better view of the situation. For that, I use \"stem_and_lem\" module in the TextAnalysis folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to make instances of the class TextProcessor to apply all the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_lose = TextPreprocessor(comments_after_lose)\n",
    "preprocessor_win = TextPreprocessor(comments_after_win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we create dataframes out of the instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body -- the name of the column that contains the text of the comments\n",
    "preprocessed_lose_df = preprocessor_lose.apply_all('body')\n",
    "preprocessed_win_df = preprocessor_win.apply_all('body')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_lose_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_win_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn to the replacement of the word that refer to the same person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We firstly create two instances of the WordReplacer class\n",
    "word_replacer_win = WordReplacer(preprocessed_win_df)\n",
    "word_replacer_lose = WordReplacer(preprocessed_lose_df)\n",
    "\n",
    "#Then we create two dataframes (I name them the same way to avoid the confusion)\n",
    "# and we use the dictionary 'word_mapping' from the respective module.\n",
    "preprocessed_win_df = word_replacer_win.replace_words('body', word_mapping)\n",
    "word_replacer_lose = word_replacer_lose.replace_words('body', word_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_win_df.head(5) #It works, as we have \"leao\" instead of \"rafa\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can proceede with adding columns with stemmed and lemmatized words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# We firstly create two instances of the TextStemLem class:\n",
    "tsl_w = TextStemLem(preprocessed_win_df)\n",
    "tsl_l = TextStemLem(preprocessed_lose_df)\n",
    "\n",
    "# Then we add a column with stemmed words\n",
    "tsl_w.stem_words('body')\n",
    "tsl_l.stem_words('body')\n",
    "\n",
    "# Finally, we add a column with lemmatized words\n",
    "tsl_w.lemmatize_words('body')\n",
    "tsl_l.lemmatize_words('body')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_win_df = tsl_w.dataframe\n",
    "preprocessed_lose_df = tsl_l.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>stemmed_words</th>\n",
       "      <th>lemmatized_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e747f9104ee94c81cea8882e72569ff2ffaf7d7d0bf760...</td>\n",
       "      <td>match thread todays match testing new feature ...</td>\n",
       "      <td>2023-11-07 17:47:32</td>\n",
       "      <td>match thread today match test new featur reddi...</td>\n",
       "      <td>match thread today match testing new feature r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c781e576544f18d1a772440f1e38b705e047cb0360936a...</td>\n",
       "      <td>rlc different type animal today holy sh****</td>\n",
       "      <td>2023-11-07 22:02:38</td>\n",
       "      <td>rlc differ type anim today holi sh****</td>\n",
       "      <td>rlc different type animal today holy sh****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7aa5347ca21d3a470856780720af01c6f2c9581185542...</td>\n",
       "      <td>leao world class tonight running like crazy</td>\n",
       "      <td>2023-11-07 21:42:52</td>\n",
       "      <td>leao world class tonight run like crazi</td>\n",
       "      <td>leao world class tonight running like crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e52216b2a493bd566000fde19217047c63ff6e1f21c1c1...</td>\n",
       "      <td>rlc signing summer confirmed</td>\n",
       "      <td>2023-11-07 20:51:49</td>\n",
       "      <td>rlc sign summer confirm</td>\n",
       "      <td>rlc signing summer confirmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>439667f389bbc1317e68675bfd5f5080d800f2fc7df6b9...</td>\n",
       "      <td>calabria pocketing superstar</td>\n",
       "      <td>2023-11-07 21:59:58</td>\n",
       "      <td>calabria pocket superstar</td>\n",
       "      <td>calabria pocketing superstar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              author  \\\n",
       "0  e747f9104ee94c81cea8882e72569ff2ffaf7d7d0bf760...   \n",
       "1  c781e576544f18d1a772440f1e38b705e047cb0360936a...   \n",
       "2  b7aa5347ca21d3a470856780720af01c6f2c9581185542...   \n",
       "3  e52216b2a493bd566000fde19217047c63ff6e1f21c1c1...   \n",
       "4  439667f389bbc1317e68675bfd5f5080d800f2fc7df6b9...   \n",
       "\n",
       "                                                body    created_datetime  \\\n",
       "0  match thread todays match testing new feature ... 2023-11-07 17:47:32   \n",
       "1        rlc different type animal today holy sh**** 2023-11-07 22:02:38   \n",
       "2        leao world class tonight running like crazy 2023-11-07 21:42:52   \n",
       "3                       rlc signing summer confirmed 2023-11-07 20:51:49   \n",
       "4                       calabria pocketing superstar 2023-11-07 21:59:58   \n",
       "\n",
       "                                       stemmed_words  \\\n",
       "0  match thread today match test new featur reddi...   \n",
       "1             rlc differ type anim today holi sh****   \n",
       "2            leao world class tonight run like crazi   \n",
       "3                            rlc sign summer confirm   \n",
       "4                          calabria pocket superstar   \n",
       "\n",
       "                                    lemmatized_words  \n",
       "0  match thread today match testing new feature r...  \n",
       "1        rlc different type animal today holy sh****  \n",
       "2        leao world class tonight running like crazy  \n",
       "3                       rlc signing summer confirmed  \n",
       "4                       calabria pocketing superstar  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_win_df.head(5) # As you can see, everything works\n",
    "# and we are done with the preprocessing part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
